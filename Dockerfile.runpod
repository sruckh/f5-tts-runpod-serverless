# Use official F5-TTS container as base - pre-built and optimized
FROM ghcr.io/swivid/f5-tts:main

# Set working directory
WORKDIR /app

# Install minimal additional dependencies for RunPod serverless
RUN pip install --no-cache-dir \
    runpod \
    boto3 \
    requests

# Copy our serverless handler and utilities
COPY runpod-handler.py /app/runpod-handler.py
COPY s3_utils.py /app/s3_utils.py
COPY model_cache_init.py /app/model_cache_init.py

# Create model cache directories
RUN mkdir -p /app/models /runpod-volume/models

# Set environment variables for optimal performance
ENV PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
ENV CUDA_VISIBLE_DEVICES=0

# Use RunPod persistent volume for model caching (fallback to local if not available)
ENV HF_HOME=/runpod-volume/models
ENV TRANSFORMERS_CACHE=/runpod-volume/models
ENV HF_HUB_CACHE=/runpod-volume/models/hub
ENV TORCH_HOME=/runpod-volume/models/torch

# Create symlink for backward compatibility  
RUN ln -sf /runpod-volume/models /app/models || true

# Install CUDA 12.4 compatible flash_attn as final step to ensure compatibility
RUN pip install --no-cache-dir --force-reinstall \
    https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.2/flash_attn-2.8.2+cu12torch2.4cxx11abiFALSE-cp310-cp310-linux_x86_64.whl

# Initialize model cache and start the serverless worker
CMD ["sh", "-c", "python /app/model_cache_init.py && python -u /app/runpod-handler.py"]